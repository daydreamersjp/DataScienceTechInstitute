{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is meant to make a python note to follow the blog post series of this:\n",
    "https://mpatacchiola.github.io/blog/2016/12/09/dissecting-reinforcement-learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking If Bellman Optimality Equation Works\n",
    "\n",
    "With the optimal policy, the state-value function satisfies the following Bellman Optimality Equation:\n",
    "\n",
    "\\begin{equation}\n",
    "v_*(s) = max_{a \\in A} (R_s^a + \\gamma \\Sigma_{s'\\in S}P_{ss'}^av_*(s'))\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9, 0.9, 0.1, 0.1],\n",
       "        [0.1, 0. , 0.1, 0.8],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0.1, 0.8, 0.1],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0.1, 0.8, 0.1, 0. ],\n",
       "        [0.8, 0.2, 0.8, 0.2],\n",
       "        [0.1, 0. , 0.1, 0.8],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0.8, 0.1, 0. ],\n",
       "        [0.8, 0.1, 0. , 0.1],\n",
       "        [0.1, 0. , 0.1, 0.8],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0.1, 0.8, 0.1],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0.8, 0.1, 0. , 0.1],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0.2, 0.8, 0.2, 0.8],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0.1, 0.8, 0.1],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0.8, 0.1, 0. , 0.1],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0.8, 0.1, 0. ],\n",
       "        [0.1, 0. , 0.1, 0.8],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0.1, 0.8, 0.1],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0.8, 0.1, 0. , 0.1],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0.9, 0.9, 0.1],\n",
       "        [0.1, 0. , 0.1, 0.8],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0.8, 0.1, 0. ],\n",
       "        [0.8, 0.2, 0.8, 0.2],\n",
       "        [0.1, 0. , 0.1, 0.8],\n",
       "        [0. , 0. , 0. , 0. ]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0.8, 0.1, 0. , 0.1],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0.8, 0.1, 0. ],\n",
       "        [0. , 0.1, 0.8, 0.1],\n",
       "        [0.1, 0. , 0.1, 0.8]],\n",
       "\n",
       "       [[0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0.8, 0.1, 0. , 0.1],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0. , 0. , 0. , 0. ],\n",
       "        [0.1, 0.8, 0.1, 0. ],\n",
       "        [0.1, 0.1, 0.9, 0.9]]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Transition matrix - 3 dimensions\n",
    "First dimention d=12: From state (including inadmissible cell #6)\n",
    "Seond dimension d=12: To state (including terminal state #4 & 8 and inadmissible cell #6)\n",
    "Third dimension d=4: Action (0-4): 0=Up, 1=Left, 2=Down, 3=Right\n",
    "\n",
    "This matrix represents the situation that the selection of action does not necessarily give deterministic moves, \n",
    "but a stochastic ones with some mis-directed moves.\n",
    "\"\"\" \n",
    "\n",
    "P = np.load(\"T.npy\")\n",
    "P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9, 0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0.1, 0.8, 0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0.1, 0.8, 0.1, 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0.8, 0. , 0. , 0. , 0.2, 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0.8, 0. , 0. , 0. , 0.1, 0.1, 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0.8, 0. , 0. , 0. , 0.1, 0.1, 0. , 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.1, 0.8, 0.1, 0. ],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0.8, 0. , 0. , 0.1, 0. , 0.1],\n",
       "       [0. , 0. , 0. , 0. , 0. , 0. , 0. , 0.8, 0. , 0. , 0.1, 0.1]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_state_vf_optimal(s,P,v,r,gamma):\n",
    "    \"\"\"\n",
    "    @return\n",
    "    state-value function at s==1 cell with the optimal policy reflected on transition probability matrix P\n",
    "    \n",
    "    @param\n",
    "    s - array with n=12: state -> 1 at the place agent exists on, 0 otherwise\n",
    "    P - 3 dim matrix with n=12x12x4: transition probability matrix on the policy\n",
    "    v - array with n=12: state-value functions\n",
    "    r - scalar: immediate reward\n",
    "    gamma - scalar: discount rate    \n",
    "    \"\"\"\n",
    "    action_array = np.zeros(4)\n",
    "    for a in range(4):\n",
    "        action_array[a] = np.sum(np.multiply(v, np.dot(s, P[:,:,a])))\n",
    "    return r + gamma * np.max(action_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Utility of state (1,1): 0.7056\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    s = np.array([[0.0, 0.0, 0.0, 0.0, \n",
    "                   0.0, 0.0, 0.0, 0.0, \n",
    "                   1.0, 0.0, 0.0, 0.0]])\n",
    "    P = np.load(\"T.npy\")\n",
    "    #State-value function with optimal policy\n",
    "    v = np.array([[0.812, 0.868, 0.918,   1.0,\n",
    "                   0.762,   0.0, 0.660,  -1.0,\n",
    "                   0.705, 0.655, 0.611, 0.388]])\n",
    "    r = -0.04\n",
    "    gamma = 1.0\n",
    "\n",
    "    utility_11 = return_state_vf_optimal(s, P, v, r, gamma)\n",
    "    print(\"Utility of state (1,1): \" + str(utility_11))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Value Iteration Algorithm\n",
    "\n",
    "Value Iteration algorithm is to get the optimal state-value functions directly by iterations such that:\n",
    "\n",
    "\\begin{equation}\n",
    "v_{k+1}(s) = max_{a \\in A} (R_s^a + \\gamma \\Sigma_{s'\\in S}P_{ss'}^av_{k}(s'))\n",
    "\\end{equation}\n",
    "\n",
    "If $\\gamma =1$, it never converges. As it gets closer to 1.0, the final numbers are close enough to the true optimal numbers.\n",
    "\n",
    "The drawback of value iteration algorithm is it does not provide the optimal policy as it is, though it is indirectly given by choosing the best action $a$ among the action-value functions derived from:\n",
    "\n",
    "\\begin{equation}\n",
    "q_{*}(s,a) = R_s^a + \\gamma \\Sigma_{s'\\in S}P_{ss'}^av_{*}(s')\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "=================== FINAL RESULT ==================\n",
      "Iterations: 41\n",
      "Delta: 9.328160466282043e-11\n",
      "Gamma: 0.9999999\n",
      "Epsilon: 0.001\n",
      "===================================================\n",
      "[0.81155786 0.86780798 0.91780809 1.        ]\n",
      "[ 0.76155776  0.          0.66027378 -1.        ]\n",
      "[0.70530765 0.65530757 0.61141485 0.38792427]\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def main():\n",
    "    #Change as you want\n",
    "    tot_states = 12\n",
    "    gamma = 0.9999999 #Discount factor\n",
    "    iteration = 0 #Iteration counter\n",
    "    epsilon = 0.001 #Stopping criteria small value\n",
    "\n",
    "    #List containing the data for each iteation\n",
    "    graph_list = list()\n",
    "\n",
    "    #Transition matrix loaded from file (It is too big to write here)\n",
    "    P = np.load(\"T.npy\")\n",
    "\n",
    "    #Reward vector\n",
    "    r = np.array([-0.04, -0.04, -0.04,  +1.0,\n",
    "                  -0.04,   0.0, -0.04,  -1.0,\n",
    "                  -0.04, -0.04, -0.04, -0.04])    \n",
    "\n",
    "    #Initial state-value function vectors\n",
    "    v_k0 = np.array([0.0, 0.0, 0.0,  0.0,\n",
    "                   0.0, 0.0, 0.0,  0.0,\n",
    "                   0.0, 0.0, 0.0,  0.0])\n",
    "    v_k1 = np.array([0.0, 0.0, 0.0,  0.0,\n",
    "                    0.0, 0.0, 0.0,  0.0,\n",
    "                    0.0, 0.0, 0.0,  0.0])\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        v_k0 = v_k1.copy()\n",
    "        iteration += 1\n",
    "        graph_list.append(v_k0)\n",
    "        for c in range(tot_states):\n",
    "            reward = r[c]\n",
    "            s = np.zeros((1,tot_states))\n",
    "            s[0,c] = 1.0\n",
    "            v_k1[c] = return_state_vf_optimal(s, P, v_k0, reward, gamma)\n",
    "            delta = max(delta, np.abs(v_k1[c] - v_k0[c])) #Stopping criteria       \n",
    "        if delta < epsilon * (1 - gamma) / gamma:\n",
    "                print(\"=================== FINAL RESULT ==================\")\n",
    "                print(\"Iterations: \" + str(iteration))\n",
    "                print(\"Delta: \" + str(delta))\n",
    "                print(\"Gamma: \" + str(gamma))\n",
    "                print(\"Epsilon: \" + str(epsilon))\n",
    "                print(\"===================================================\")\n",
    "                print(v_k0[0:4])\n",
    "                print(v_k0[4:8])\n",
    "                print(v_k0[8:12])\n",
    "                print(\"===================================================\")\n",
    "                break\n",
    "#     print(graph_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation + Policy Iteration\n",
    "Iterative Policy Evaluation + Policy Iteration is another iterative approach to obtain the optimal state-value functions and optimal policy with alternation of evaluation of policy and exploration of best policy (by greedy algorithm) with the recent evaluation.\n",
    "\n",
    "<br>\n",
    "\n",
    "Here's one iteration of Iterative Policy Evaluation:\n",
    "\n",
    "\\begin{equation}\n",
    "v_{k+1}(s) = \\Sigma_{a \\in A} \\pi_k(a|s) (R_s^a + \\gamma \\Sigma_{s'\\in S}P_{ss'}^av_{k}(s'))\n",
    "\\end{equation}\n",
    "\n",
    "Repeating this until the state-value function conversges to the state-VF at cycle $t$ as $v_{t}(s)$ based on the current policy.\n",
    "\n",
    "<br>\n",
    "\n",
    "Here's Policy Iteration using evaluated cycle $t$ state-value function $v_{t}(s)$ (greedy algorithm):\n",
    "\n",
    "\\begin{equation}\n",
    "\\newcommand{\\argmax}{\\mathop{\\rm arg~max}\\limits}\n",
    "\\newcommand{\\argmin}{\\mathop{\\rm arg~min}\\limits}\n",
    "\\pi_{t+1}(a|s) = \\argmax_a (R_s^a + \\gamma \\Sigma_{s'\\in S} P_{ss'}^av_{t}(s'))\n",
    "\\end{equation}\n",
    "\n",
    "<br>\n",
    "\n",
    "Repeating Iterative Policy Evaluation and Policy Iteration, reach at the optimal policy and state-value function:\n",
    "\n",
    "\\begin{equation}\n",
    "\\pi_{1}(a|s) \\rightarrow v_{1}(s) \\rightarrow \\pi_{2}(a|s) \\rightarrow v_{2}(s) \\rightarrow ... \\rightarrow \\pi_{t}(a|s) \\rightarrow v_{t}(s) \\rightarrow \\pi_{t+1}(a|s) \\rightarrow ... \\rightarrow  \\pi_{*}(a|s) \\rightarrow v_{*}(s)\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(s,P,v,r,gamma,po):\n",
    "    \"\"\"\n",
    "    @return: a scalar value\n",
    "    converged state-value function at s==1 cell with the optimal policy reflected on transition probability matrix P\n",
    "    \n",
    "    @param\n",
    "    s - array with n=12: state -> 1 at the place agent exists on, 0 otherwise\n",
    "    P - 3 dim matrix with n=12x12x4: transition probability matrix on the policy\n",
    "    v - array with n=12: state-value functions\n",
    "    r - scalar: immediate reward\n",
    "    gamma - scalar: discount rate  \n",
    "    po - array with n=12x4: current policy representing the probability of taking action 0=Up, 1=Left, 2=Down, 3=Right\n",
    "    \"\"\"    \n",
    "    action_array = np.zeros(4)\n",
    "    for a in range(4):\n",
    "        action_array[a] = np.sum(np.multiply(v, np.dot(s, P[:,:,a])))\n",
    "    return r + gamma * np.sum(np.multiply(action_array, np.dot(s, po)))\n",
    "    \n",
    "\n",
    "def policy_iteration(s,P,v,r,gamma):\n",
    "    \"\"\"\n",
    "    @return: an array with n=4\n",
    "    probabilities of taking actions with indices 0=Up, 1=Left, 2=Down, 3=Right\n",
    "    \n",
    "    @param\n",
    "    s - array with n=12: state -> 1 at the place agent exists on, 0 otherwise\n",
    "    P - 3 dim matrix with n=12x12x4: transition probability matrix on the policy\n",
    "    v - array with n=12: state-value functions\n",
    "    r - scalar: immediate reward\n",
    "    gamma - scalar: discount rate  \n",
    "    \"\"\"        \n",
    "    action_array = np.zeros(4)\n",
    "    for a in range(4):\n",
    "        action_array[a] = np.sum(np.multiply(v, np.dot(s, P[:,:,a])))\n",
    "    po_array = np.zeros(4)\n",
    "    for a in range(4):\n",
    "        if action_array[a]==np.max(action_array):\n",
    "            po_array[a]=1\n",
    "    return po_array / np.sum(po_array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "=================== FINAL RESULT ==================\n",
      "Iterations: 4\n",
      "Gamma: 0.9999999\n",
      "Epsilon for state-VF: 0.001\n",
      "===================================================\n",
      "[0.81155786 0.86780798 0.91780809 1.        ]\n",
      "[ 0.76155776  0.          0.66027378 -1.        ]\n",
      "[0.70530765 0.65530757 0.61141485 0.38792427]\n",
      "===================================================\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "def main():\n",
    "    #Change as you want\n",
    "    tot_states = 12\n",
    "    gamma = 0.9999999 #Discount factor\n",
    "    iteration = 0 #Iteration counter\n",
    "    epsilon_v = 0.001 #Stopping criteria of iterative policy evaluation\n",
    "    epsilon_p = 0.001 #Stopping criteria of policy iteration \n",
    "\n",
    "    #Transition matrix loaded from file (It is too big to write here)\n",
    "    P = np.load(\"T.npy\")\n",
    "\n",
    "    #Reward vector\n",
    "    r = np.array([-0.04, -0.04, -0.04,  +1.0,\n",
    "                  -0.04,   0.0, -0.04,  -1.0,\n",
    "                  -0.04, -0.04, -0.04, -0.04])    \n",
    "\n",
    "    #Initial state-value function vectors\n",
    "#     v_k0 = np.array([0.0, 0.0, 0.0,  0.0,\n",
    "#                    0.0, 0.0, 0.0,  0.0,\n",
    "#                    0.0, 0.0, 0.0,  0.0])\n",
    "    v_k1 = np.array([0.0, 0.0, 0.0,  0.0,\n",
    "                    0.0, 0.0, 0.0,  0.0,\n",
    "                    0.0, 0.0, 0.0,  0.0])\n",
    "    \n",
    "    #Initial policy matrix\n",
    "    po_k1 = [[0. , 0. , 0.5, 0.5],\n",
    "            [0. , 0.33,0.33,0.34],\n",
    "            [0. , 0.33,0.33,0.34],\n",
    "            [0. , 0. , 0. , 0. ],\n",
    "            [0.5, 0. , 0.5, 0. ],\n",
    "            [0. , 0. , 0. , 0. ],\n",
    "            [0.33,0. ,0.33, 0.34],\n",
    "            [0. , 0. , 0. , 0. ],\n",
    "            [0.5, 0. , 0. , 0.5],\n",
    "            [0. , 0.5, 0. , 0.5],\n",
    "            [0.33,0.33, 0. ,0.33],\n",
    "            [0.5, 0.5, 0. , 0. ]]\n",
    "\n",
    "\n",
    "    while True:       \n",
    "        delta_p = 0\n",
    "        po_k0 = po_k1.copy()\n",
    "        iteration += 1\n",
    "    \n",
    "        while True:\n",
    "            delta_v = 0\n",
    "            v_k0 = v_k1.copy()\n",
    "#             graph_list.append(v_k0)\n",
    "            for c in range(tot_states):\n",
    "                reward = r[c]\n",
    "                s = np.zeros((1,tot_states))\n",
    "                s[0,c] = 1.0\n",
    "                v_k1[c] = policy_evaluation(s, P, v_k0, reward, gamma, po_k0)\n",
    "                delta_v = max(delta_v, np.abs(v_k1[c] - v_k0[c])) #Stopping criteria   \n",
    "\n",
    "            if delta_v < epsilon_v * (1 - gamma) / gamma:\n",
    "                break\n",
    "                \n",
    "        for c in range(tot_states):\n",
    "            reward = r[c]\n",
    "            s = np.zeros((1,tot_states))\n",
    "            s[0,c] = 1.0\n",
    "            po_k1[c] = policy_iteration(s, P, v_k1, reward, gamma)\n",
    "            for a in range(4):\n",
    "                delta_p = max(delta_p, np.abs(po_k1[c][a] - po_k0[c][a])) #Stopping criteria  \n",
    "            \n",
    "        if delta_p < epsilon_p:\n",
    "            print(\"=================== FINAL RESULT ==================\")\n",
    "            print(\"Iterations: \" + str(iteration))\n",
    "#             print(\"Delta: \" + str(delta))\n",
    "            print(\"Gamma: \" + str(gamma))\n",
    "            print(\"Epsilon for state-VF: \" + str(epsilon_v))\n",
    "            print(\"===================================================\")\n",
    "            print(v_k1[0:4])\n",
    "            print(v_k1[4:8])\n",
    "            print(v_k1[8:12])\n",
    "            print(\"===================================================\")\n",
    "            break            \n",
    "            \n",
    "        \n",
    "        \n",
    "#     print(graph_list)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Results of Value Iteration Algorithm:\\n=================== FINAL RESULT ==================\\nIterations: 41\\nDelta: 9.328160466282043e-11\\nGamma: 0.9999999\\nEpsilon: 0.001\\n===================================================\\n[0.81155786 0.86780798 0.91780809 1.        ]\\n[ 0.76155776  0.          0.66027378 -1.        ]\\n[0.70530765 0.65530757 0.61141485 0.38792427]\\n===================================================\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' Results of Value Iteration Algorithm:\n",
    "=================== FINAL RESULT ==================\n",
    "Iterations: 41\n",
    "Delta: 9.328160466282043e-11\n",
    "Gamma: 0.9999999\n",
    "Epsilon: 0.001\n",
    "===================================================\n",
    "[0.81155786 0.86780798 0.91780809 1.        ]\n",
    "[ 0.76155776  0.          0.66027378 -1.        ]\n",
    "[0.70530765 0.65530757 0.61141485 0.38792427]\n",
    "===================================================\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
